{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"ViT-B-32\"\n",
        "PRETRAINED_DATASET = \"laion2b_s34b_b79k\"\n",
        "DEVICE_PREFERENCE = \"mps\"  # change to \"cuda\" or \"cpu\" if needed\n",
        "SEED = 42\n",
        "\n",
        "RUN_SPLITS = [\"train\", \"test\"]  # adjust to limit splits\n",
        "LIMIT_IMAGES = 1000  # lower for quick smoke tests\n",
        "BATCH_SIZE = 32\n",
        "K_VALUES = (1, 5, 10)\n",
        "\n",
        "TEXT_CHUNK_WORDS = 8  # set 0 to disable chunking\n",
        "TEXT_CHUNK_STRIDE = 4\n",
        "TEXT_POOLING = \"mean\"  # mean / max / attn\n",
        "TEXT_CHUNK_THRESHOLD = 32  # captions shorter than this keep original form unless entities are used\n",
        "\n",
        "ADAPTER_STEPS = 100\n",
        "ADAPTER_LR = 1e-5\n",
        "ADAPTER_LOGIT_LR = 5e-7\n",
        "ADAPTER_BATCH = 32\n",
        "ADAPTER_HIDDEN = 64\n",
        "DISTILL_WEIGHTS = [0.0, 0.2, 0.5, 1.0]\n",
        "ENTITY_PHRASE_MIN_WORDS = 2\n",
        "\n",
        "ENTITIES_ROOT = \"data/flickr30k/flickr30k_entities-master\"\n",
        "IMAGES_ROOT = \"data/flickr30k/flickr30k-images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, cast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "\n",
        "entities_python_root = Path(ENTITIES_ROOT).resolve()\n",
        "if str(entities_python_root) not in sys.path:\n",
        "    sys.path.append(str(entities_python_root))\n",
        "\n",
        "from flickr30k_entities_utils import get_sentence_data  # type: ignore[import]\n",
        "from adapter import attach_text_adapters, detach_text_adapters, normalize_features\n",
        "from utils import (\n",
        "    encode_openclip_embeddings,\n",
        "    ensure_dir,\n",
        "    load_flickr30k_karpathy_json,\n",
        "    pool_chunk_embeddings,\n",
        "    chunk_caption_words,\n",
        "    recall_at_k_image_to_text,\n",
        "    recall_at_k_text_to_image,\n",
        "    select_torch_device,\n",
        "    set_all_seeds,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root: /Users/apple/grad_material/2025 fall/comp 545/final\n",
            "output_dir: /Users/apple/grad_material/2025 fall/comp 545/final/output\n",
            "device: mps\n"
          ]
        }
      ],
      "source": [
        "# Paths, device, seeds\n",
        "root = Path(os.getcwd()).resolve()\n",
        "output_dir = root / \"output\"\n",
        "ensure_dir(output_dir)\n",
        "plot_dir = output_dir / \"tmp\"\n",
        "ensure_dir(plot_dir)\n",
        "device = select_torch_device(DEVICE_PREFERENCE)\n",
        "set_all_seeds(SEED)\n",
        "print(f\"root: {root}\")\n",
        "print(f\"output_dir: {output_dir}\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reminder: make sure the annotation JSON files already exist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CLIP model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 63,428,097\n"
          ]
        }
      ],
      "source": [
        "import open_clip\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "    MODEL_NAME,\n",
        "    pretrained=PRETRAINED_DATASET,\n",
        "    device=device,\n",
        ")\n",
        "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
        "model.eval()\n",
        "\n",
        "visual_module = cast(torch.nn.Module, model.visual)\n",
        "for p in visual_module.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "image_transform = cast(Callable[[Image.Image], torch.Tensor], preprocess)\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f\"trainable params: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Karpathy train/test splits\n",
        "\n",
        "def load_split(split: str) -> List[Dict[str, object]]:\n",
        "    ann_path = output_dir / f\"flickr30k_annotations_{split}.json\"\n",
        "    return load_flickr30k_karpathy_json(\n",
        "        annotations_path=ann_path,\n",
        "        images_root=Path(IMAGES_ROOT),\n",
        "        split=split,\n",
        "        limit=LIMIT_IMAGES,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split=train images=1000 captions=5000\n",
            "split=test images=1000 captions=5000\n"
          ]
        }
      ],
      "source": [
        "datasets: Dict[str, List[Dict[str, object]]] = {}\n",
        "for split in RUN_SPLITS:\n",
        "    split_data = load_split(split)\n",
        "    datasets[split] = split_data\n",
        "    num_caps = sum(len(cast(List[str], item[\"captions\"])) for item in split_data)\n",
        "    print(f\"split={split} images={len(split_data)} captions={num_caps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_entity_phrase_cache(dataset: List[Dict[str, object]]) -> Dict[str, List[Dict[str, object]]]:\n",
        "    cache: Dict[str, List[Dict[str, object]]] = {}\n",
        "    sentences_root = Path(ENTITIES_ROOT) / \"annotations\" / \"Sentences\"\n",
        "    for item in dataset:\n",
        "        image_path = Path(str(item[\"image_path\"]))\n",
        "        image_id = image_path.stem\n",
        "        sentence_file = sentences_root / f\"{image_id}.txt\"\n",
        "        if sentence_file.is_file():\n",
        "            try:\n",
        "                cache[image_id] = get_sentence_data(str(sentence_file))\n",
        "            except Exception:\n",
        "                cache[image_id] = []\n",
        "        else:\n",
        "            cache[image_id] = []\n",
        "    return cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def deduplicate_preserve_order(items: List[str]) -> List[str]:\n",
        "    seen = set()\n",
        "    ordered: List[str] = []\n",
        "    for item in items:\n",
        "        key = item.strip()\n",
        "        if not key:\n",
        "            continue\n",
        "        norm = key.lower()\n",
        "        if norm in seen:\n",
        "            continue\n",
        "        seen.add(norm)\n",
        "        ordered.append(key)\n",
        "    return ordered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_semantic_segments(\n",
        "    image_id: str,\n",
        "    caption_idx: int,\n",
        "    caption: str,\n",
        "    entity_cache: Dict[str, List[Dict[str, object]]],\n",
        ") -> List[str]:\n",
        "    caption = caption.strip()\n",
        "    words = caption.split()\n",
        "    segments: List[str] = []\n",
        "    if TEXT_CHUNK_WORDS > 0 and len(words) >= TEXT_CHUNK_THRESHOLD:\n",
        "        segments.extend(\n",
        "            chunk_caption_words(\n",
        "                caption,\n",
        "                TEXT_CHUNK_WORDS,\n",
        "                TEXT_CHUNK_STRIDE,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    sentence_data = entity_cache.get(image_id)\n",
        "    if sentence_data and caption_idx < len(sentence_data):\n",
        "        sentence_entry = sentence_data[caption_idx]\n",
        "        phrase_entries = cast(List[Dict[str, Any]], sentence_entry.get(\"phrases\", []))\n",
        "        attribute_phrases = [\n",
        "            str(p.get(\"phrase\", \"\")).strip()\n",
        "            for p in phrase_entries\n",
        "            if len(str(p.get(\"phrase\", \"\")).split()) >= ENTITY_PHRASE_MIN_WORDS\n",
        "        ]\n",
        "        if attribute_phrases:\n",
        "            if not segments:\n",
        "                segments.extend(attribute_phrases)\n",
        "            else:\n",
        "                segments.extend(attribute_phrases)\n",
        "\n",
        "    if not segments:\n",
        "        segments = [caption]\n",
        "    else:\n",
        "        if caption not in segments:\n",
        "            segments.append(caption)\n",
        "    return deduplicate_preserve_order(segments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_chunked_embeddings(\n",
        "    split: str,\n",
        "    entity_cache: Dict[str, List[Dict[str, object]]],\n",
        "    *,\n",
        "    progress_desc: str,\n",
        ") -> Dict[str, object]:\n",
        "    if progress_desc:\n",
        "        print(f\"encoding {progress_desc}...\")\n",
        "    dataset = datasets[split]\n",
        "    image_paths = [str(item[\"image_path\"]) for item in dataset]\n",
        "\n",
        "    image_embeddings: List[torch.Tensor] = []\n",
        "    for start in range(0, len(image_paths), BATCH_SIZE):\n",
        "        batch_paths = image_paths[start:start + BATCH_SIZE]\n",
        "        images: List[torch.Tensor] = []\n",
        "        for path in batch_paths:\n",
        "            with Image.open(path).convert(\"RGB\") as img:\n",
        "                images.append(image_transform(img))\n",
        "        image_tensor = torch.stack(images, dim=0).to(device)\n",
        "        with torch.no_grad():\n",
        "            feats = model.encode_image(image_tensor).float()  # type: ignore[attr-defined]\n",
        "        image_embeddings.append(normalize_features(feats).cpu())\n",
        "    image_embed_tensor = torch.cat(image_embeddings, dim=0)\n",
        "\n",
        "    caption_info: List[Tuple[int, int, int]] = []  # (image_idx, start, end)\n",
        "    segment_texts: List[str] = []\n",
        "    for image_idx, item in enumerate(dataset):\n",
        "        image_id = Path(str(item[\"image_path\"])).stem\n",
        "        captions = cast(List[str], item[\"captions\"])\n",
        "        for caption_idx, caption in enumerate(captions):\n",
        "            segments = build_semantic_segments(image_id, caption_idx, caption, entity_cache)\n",
        "            start = len(segment_texts)\n",
        "            segment_texts.extend(segments)\n",
        "            end = len(segment_texts)\n",
        "            caption_info.append((image_idx, start, end))\n",
        "\n",
        "    if not segment_texts:\n",
        "        raise RuntimeError(\"No segments generated for chunked encoding\")\n",
        "\n",
        "    segment_embeds: List[torch.Tensor] = []\n",
        "    for start in range(0, len(segment_texts), BATCH_SIZE):\n",
        "        texts = segment_texts[start:start + BATCH_SIZE]\n",
        "        tokens = tokenizer(texts).to(device)\n",
        "        with torch.no_grad():\n",
        "            feats = model.encode_text(tokens).float()  # type: ignore[attr-defined]\n",
        "        segment_embeds.append(normalize_features(feats).cpu())\n",
        "    segment_matrix = torch.cat(segment_embeds, dim=0)\n",
        "\n",
        "    caption_embeddings: List[np.ndarray] = []\n",
        "    caption_to_image_index: List[int] = []\n",
        "    image_to_caption_indices: List[List[int]] = []\n",
        "\n",
        "    caption_ptr = 0\n",
        "    for image_idx, item in enumerate(dataset):\n",
        "        indices: List[int] = []\n",
        "        captions = cast(List[str], item[\"captions\"])\n",
        "        for _ in captions:\n",
        "            img_idx, start, end = caption_info[caption_ptr]\n",
        "            chunk_tensor = segment_matrix[start:end]\n",
        "            pooled = pool_chunk_embeddings(chunk_tensor, mode=TEXT_POOLING)\n",
        "            caption_embeddings.append(pooled.cpu().numpy())\n",
        "            caption_to_image_index.append(img_idx)\n",
        "            indices.append(len(caption_embeddings) - 1)\n",
        "            caption_ptr += 1\n",
        "        image_to_caption_indices.append(indices)\n",
        "\n",
        "    if caption_ptr != len(caption_info):\n",
        "        raise RuntimeError(\"Caption pointer did not consume all segment info\")\n",
        "\n",
        "    return {\n",
        "        \"image_embeddings\": image_embed_tensor.numpy(),\n",
        "        \"caption_embeddings\": np.asarray(caption_embeddings, dtype=np.float32),\n",
        "        \"caption_to_image_index\": np.asarray(caption_to_image_index, dtype=np.int64),\n",
        "        \"image_to_caption_indices\": image_to_caption_indices,\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "building entity phrase caches...\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\n",
        "entity_caches: Dict[str, Dict[str, List[Dict[str, object]]]] = {}\n",
        "if TEXT_CHUNK_WORDS > 0:\n",
        "    print(\"building entity phrase caches...\")\n",
        "    for split, data in datasets.items():\n",
        "        entity_caches[split] = build_entity_phrase_cache(data)\n",
        "else:\n",
        "    for split in datasets:\n",
        "        entity_caches[split] = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encode baseline CLIP features (run once per configuration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_clip_encoding(\n",
        "    split: str,\n",
        "    pooling: Optional[str] = None,\n",
        "    chunk_words: Optional[int] = None,\n",
        "    chunk_stride: Optional[int] = None,\n",
        ") -> Dict[str, object]:\n",
        "    words = chunk_words if chunk_words and chunk_words > 0 else None\n",
        "    stride = chunk_stride if chunk_stride and chunk_stride > 0 else None\n",
        "    if words is not None:\n",
        "        threshold = TEXT_CHUNK_THRESHOLD if TEXT_CHUNK_THRESHOLD > 0 else 0\n",
        "    else:\n",
        "        threshold = 10_000  # effectively disable chunking\n",
        "    return encode_openclip_embeddings(\n",
        "        dataset=datasets[split],\n",
        "        preprocess=preprocess,\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        device=device,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        text_chunk_words=words,\n",
        "        text_chunk_stride=stride,\n",
        "        text_pooling=pooling or TEXT_POOLING,\n",
        "        progress=True,\n",
        "        progress_desc=f\"{split}|chunk{words or 0}_{pooling or TEXT_POOLING}\",\n",
        "        chunk_threshold_tokens=threshold,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'images': 1000, 'captions': 5000}\n"
          ]
        }
      ],
      "source": [
        "def compute_recalls(embeddings: Dict[str, object]) -> Dict[str, Dict[int, float]]:\n",
        "    img = np.asarray(embeddings[\"image_embeddings\"], dtype=np.float32)\n",
        "    cap = np.asarray(embeddings[\"caption_embeddings\"], dtype=np.float32)\n",
        "    cap2img = np.asarray(embeddings[\"caption_to_image_index\"], dtype=np.int64)\n",
        "    img2cap = cast(List[List[int]], embeddings[\"image_to_caption_indices\"])\n",
        "    return {\n",
        "        \"t2i\": recall_at_k_text_to_image(img, cap, cap2img, ks=K_VALUES),\n",
        "        \"i2t\": recall_at_k_image_to_text(img, cap, img2cap, ks=K_VALUES),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_original_embeddings: Dict[str, Dict[str, object]] = {}\n",
        "baseline_original_metrics: Dict[str, Dict[str, Dict[int, float]]] = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# computing original CLIP baseline (no chunk)\n",
        "for split in RUN_SPLITS:\n",
        "    emb_orig = run_clip_encoding(\n",
        "        split,\n",
        "        pooling=\"mean\",\n",
        "        chunk_words=0,\n",
        "        chunk_stride=0,\n",
        "    )\n",
        "    metrics_orig = compute_recalls(emb_orig)\n",
        "    baseline_original_embeddings[split] = emb_orig\n",
        "    baseline_original_metrics[split] = metrics_orig\n",
        "    print(f\"baseline original {split}:\")\n",
        "    for tag in (\"t2i\", \"i2t\"):\n",
        "        for k in K_VALUES:\n",
        "            print(f\"  {tag}@{k}: {metrics_orig[tag][k] * 100:.2f}\")\n",
        "\n",
        "chunk_baseline_embeddings: Dict[str, Dict[str, object]] = {}\n",
        "chunk_baseline_metrics: Dict[str, Dict[str, Dict[int, float]]] = {}\n",
        "\n",
        "if TEXT_CHUNK_WORDS > 0:\n",
        "    print(\"computing chunked baseline (no adapter)...\")\n",
        "    for split in RUN_SPLITS:\n",
        "        emb_chunk = encode_chunked_embeddings(\n",
        "            split,\n",
        "            entity_caches[split],\n",
        "            progress_desc=f\"{split}|chunk-baseline\",\n",
        "        )\n",
        "        metrics_chunk = compute_recalls(emb_chunk)\n",
        "        chunk_baseline_embeddings[split] = emb_chunk\n",
        "        chunk_baseline_metrics[split] = metrics_chunk\n",
        "        print(f\"baseline chunk {split}:\")\n",
        "        for tag in (\"t2i\", \"i2t\"):\n",
        "            for k in K_VALUES:\n",
        "                print(f\"  {tag}@{k}: {metrics_chunk[tag][k] * 100:.2f}\")\n",
        "else:\n",
        "    chunk_baseline_embeddings = baseline_original_embeddings\n",
        "    chunk_baseline_metrics = baseline_original_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: visualize baseline recall numbers\n",
        "baseline_rows: List[Dict[str, object]] = []\n",
        "\n",
        "def append_rows(metrics: Dict[str, Dict[str, Dict[int, float]]], label: str) -> None:\n",
        "    for split, metric in metrics.items():\n",
        "        for tag in (\"t2i\", \"i2t\"):\n",
        "            for k in K_VALUES:\n",
        "                baseline_rows.append(\n",
        "                    {\n",
        "                        \"variant\": label,\n",
        "                        \"split\": split,\n",
        "                        \"metric\": f\"{tag}@{k}\",\n",
        "                        \"recall\": metric[tag][k] * 100.0,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "\n",
        "append_rows(baseline_original_metrics, \"original\")\n",
        "if TEXT_CHUNK_WORDS > 0:\n",
        "    append_rows(chunk_baseline_metrics, \"chunk\")\n",
        "\n",
        "baseline_df = pd.DataFrame(baseline_rows)\n",
        "\n",
        "for variant in baseline_df[\"variant\"].unique():\n",
        "    pivot = baseline_df[baseline_df[\"variant\"] == variant].pivot(\n",
        "        index=\"split\",\n",
        "        columns=\"metric\",\n",
        "        values=\"recall\",\n",
        "    )\n",
        "    print(f\"{variant} baseline\\n{pivot}\")\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    for split in pivot.index:\n",
        "        ax.plot(pivot.columns, pivot.loc[split], marker=\"o\", label=split)\n",
        "    ax.set_ylabel(\"Recall (%)\")\n",
        "    ax.grid(alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.legend()\n",
        "    # fig.savefig(plot_dir / f\"baseline_{variant}.png\")\n",
        "    # plt.close(fig)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "131d028fac36409cbb9bedf7823446b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test|mean:   0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1dd164024d54a4fa2d0eaa18dd0716b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test|mean:   0%|          | 0/79 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'image_embeddings': (1000, 512), 'caption_embeddings': (5000, 512)}\n",
            "Text-to-Image Recall@K: {1: 69.76, 5: 89.68, 10: 93.76}\n",
            "Image-to-Text Recall@K: {1: 84.5, 5: 96.0, 10: 98.5}\n",
            "{'saved csv': '/Users/apple/grad_material/2025 fall/comp 545/final/output/metrics_test_cw0_cs0_mean.csv'}\n"
          ]
        }
      ],
      "source": [
        "# Adapter training and evaluation across distillation weights\n",
        "teacher_caps_full: Optional[torch.Tensor] = None\n",
        "teacher_img2cap_full: Optional[List[List[int]]] = None\n",
        "if any(w > 0 for w in DISTILL_WEIGHTS):\n",
        "    teacher_source = chunk_baseline_embeddings if TEXT_CHUNK_WORDS > 0 else baseline_original_embeddings\n",
        "    teacher_caps_full = normalize_features(\n",
        "        torch.tensor(\n",
        "            teacher_source[\"train\"][\"caption_embeddings\"],\n",
        "            dtype=torch.float32,\n",
        "        device=device,\n",
        "        )\n",
        "    )\n",
        "    teacher_img2cap_full = cast(\n",
        "        List[List[int]],\n",
        "        teacher_source[\"train\"][\"image_to_caption_indices\"],\n",
        "    )\n",
        "\n",
        "results_summary: List[Dict[str, object]] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for distill_weight in DISTILL_WEIGHTS:\n",
        "    print(f\"\\n=== distill_weight={distill_weight} ===\")\n",
        "\n",
        "    detach_text_adapters(model)\n",
        "    adapter = attach_text_adapters(model, ADAPTER_HIDDEN)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad_(False)\n",
        "    for p in adapter.parameters():\n",
        "        p.requires_grad_(True)\n",
        "\n",
        "    logit_scale = cast(torch.nn.Parameter, model.logit_scale)\n",
        "    logit_scale.requires_grad_(True)\n",
        "\n",
        "    adapter_params = [p for p in adapter.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [\n",
        "            {\"params\": adapter_params, \"lr\": ADAPTER_LR, \"weight_decay\": 1e-4},\n",
        "            {\"params\": [logit_scale], \"lr\": ADAPTER_LOGIT_LR, \"weight_decay\": 0.0},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    teacher_caps = teacher_caps_full if distill_weight > 0 else None\n",
        "    teacher_img2cap = teacher_img2cap_full if distill_weight > 0 else None\n",
        "\n",
        "    train_indices = list(range(len(datasets[\"train\"])))\n",
        "    loss_log: List[Tuple[int, float, float]] = []\n",
        "\n",
        "    for step in range(ADAPTER_STEPS):\n",
        "        batch_idx = random.sample(train_indices, k=min(ADAPTER_BATCH, len(train_indices)))\n",
        "        captions: List[str] = []\n",
        "        images: List[torch.Tensor] = []\n",
        "        caption_segments: List[str] = []\n",
        "        caption_spans: List[Tuple[int, int]] = []\n",
        "        teacher_pick: List[int] = []\n",
        "        for idx in batch_idx:\n",
        "            entry = datasets[\"train\"][idx]\n",
        "            caption_list = cast(List[str], entry[\"captions\"])\n",
        "            cap_pos = random.randrange(len(caption_list))\n",
        "            selected_caption = caption_list[cap_pos]\n",
        "            captions.append(selected_caption)\n",
        "            image_path = Path(str(entry[\"image_path\"]))\n",
        "            with Image.open(image_path).convert(\"RGB\") as img:\n",
        "                images.append(image_transform(img))\n",
        "\n",
        "            image_id = image_path.stem\n",
        "            segments = build_semantic_segments(image_id, cap_pos, selected_caption, entity_caches[\"train\"])\n",
        "            start = len(caption_segments)\n",
        "            caption_segments.extend(segments)\n",
        "            caption_spans.append((start, len(caption_segments)))\n",
        "\n",
        "            if teacher_img2cap is not None:\n",
        "                teacher_pick.append(teacher_img2cap[idx][cap_pos])\n",
        "\n",
        "        image_tensor = torch.stack(images, dim=0).to(device)\n",
        "        with torch.no_grad():\n",
        "            encoded_images = model.encode_image(image_tensor).float()  # type: ignore[attr-defined]\n",
        "        image_features = normalize_features(encoded_images)\n",
        "\n",
        "        if not caption_segments:\n",
        "            raise RuntimeError(\"No caption segments generated for training batch\")\n",
        "\n",
        "        segment_tokens = tokenizer(caption_segments).to(device)\n",
        "        segment_embeds = model.encode_text(segment_tokens).float()  # type: ignore[attr-defined]\n",
        "        segment_embeds = normalize_features(segment_embeds)\n",
        "        pooled_text: List[torch.Tensor] = []\n",
        "        for start, end in caption_spans:\n",
        "            pooled_text.append(pool_chunk_embeddings(segment_embeds[start:end], mode=TEXT_POOLING))\n",
        "        text_features = torch.stack(pooled_text, dim=0)\n",
        "\n",
        "        logits = (text_features @ image_features.t()) * logit_scale.exp()\n",
        "        targets = torch.arange(logits.size(0), device=device)\n",
        "        loss = (F.cross_entropy(logits, targets) + F.cross_entropy(logits.t(), targets)) * 0.5\n",
        "\n",
        "        if teacher_caps is not None and teacher_pick:\n",
        "            idx_tensor = torch.tensor(teacher_pick, dtype=torch.long, device=device)\n",
        "            teacher_batch = teacher_caps[idx_tensor]\n",
        "            loss = loss + distill_weight * F.mse_loss(text_features, teacher_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(adapter_params + [logit_scale], max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        logit_scale.data.clamp_(min=math.log(1 / 20), max=math.log(50))\n",
        "\n",
        "        if step % 10 == 0 or step == ADAPTER_STEPS - 1:\n",
        "            loss_log.append((step, float(loss.item()), float(logit_scale.exp().item())))\n",
        "            print(f\"step {step:03d} loss={loss.item():.4f} scale={logit_scale.exp().item():.2f}\")\n",
        "\n",
        "    if loss_log:\n",
        "        steps, losses, scales = zip(*loss_log)\n",
        "        fig, ax1 = plt.subplots(figsize=(6, 3))\n",
        "        ax1.plot(steps, losses, marker=\"o\", label=\"loss\")\n",
        "        ax1.set_ylabel(\"loss\")\n",
        "        ax2 = ax1.twinx()\n",
        "        ax2.plot(steps, scales, color=\"orange\", label=\"scale\")\n",
        "        ax2.set_ylabel(\"logit scale\")\n",
        "        ax1.set_xlabel(\"step\")\n",
        "        ax1.grid(alpha=0.3)\n",
        "        fig.legend(loc=\"upper right\")\n",
        "        plt.tight_layout()\n",
        "        fig.savefig(plot_dir / f\"loss_distill_{distill_weight:.2f}.png\")\n",
        "        plt.close(fig)\n",
        "\n",
        "    adapter_metrics: Dict[str, Dict[str, Dict[int, float]]] = {}\n",
        "    for split in RUN_SPLITS:\n",
        "        if TEXT_CHUNK_WORDS > 0:\n",
        "            emb = encode_chunked_embeddings(\n",
        "                split,\n",
        "                entity_caches[split],\n",
        "                progress_desc=f\"{split}|adapter_{distill_weight}\",\n",
        "            )\n",
        "        else:\n",
        "            emb = run_clip_encoding(\n",
        "                split,\n",
        "                pooling=\"mean\",\n",
        "                chunk_words=0,\n",
        "                chunk_stride=0,\n",
        "            )\n",
        "        metrics = compute_recalls(emb)\n",
        "        adapter_metrics[split] = metrics\n",
        "        print(f\"adapter (distill={distill_weight}) {split}:\")\n",
        "        for tag in (\"t2i\", \"i2t\"):\n",
        "            for k in K_VALUES:\n",
        "                print(f\"  {tag}@{k}: {metrics[tag][k] * 100:.2f}\")\n",
        "\n",
        "    for tag in (\"t2i\", \"i2t\"):\n",
        "        for k in K_VALUES:\n",
        "            results_summary.append(\n",
        "                {\n",
        "                    \"distill\": distill_weight,\n",
        "                    \"metric\": f\"{tag}@{k}\",\n",
        "                    \"adapter\": adapter_metrics[\"test\"][tag][k] * 100.0,\n",
        "                    \"baseline_original\": baseline_original_metrics[\"test\"][tag][k] * 100.0,\n",
        "                    \"baseline_chunk\": chunk_baseline_metrics[\"test\"][tag][k] * 100.0,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    detach_text_adapters(model)\n",
        "    logit_scale.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_df = pd.DataFrame(results_summary)\n",
        "print(\"\\nSummary (test split):\")\n",
        "print(\n",
        "    summary_df.pivot_table(\n",
        "        index=\"metric\",\n",
        "        columns=\"distill\",\n",
        "        values=\"adapter\",\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_order = sorted(summary_df[\"metric\"].unique())\n",
        "variant_to_values: Dict[str, List[float]] = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_metric_values(metric_source: Dict[str, Dict[int, float]]) -> List[float]:\n",
        "    values: List[float] = []\n",
        "    for metric in metrics_order:\n",
        "        tag, k_str = metric.split(\"@\")\n",
        "        values.append(metric_source[tag][int(k_str)] * 100.0)\n",
        "    return values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variant_to_values[\"baseline_original\"] = extract_metric_values(baseline_original_metrics[\"test\"])\n",
        "variant_to_values[\"baseline_chunk\"] = extract_metric_values(chunk_baseline_metrics[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for weight in DISTILL_WEIGHTS:\n",
        "    adapter_rows = summary_df[summary_df[\"distill\"] == weight]\n",
        "    adapter_map = {row[\"metric\"]: float(row[\"adapter\"]) for _, row in adapter_rows.iterrows()}\n",
        "    variant_to_values[f\"adapter_{weight:.2f}\"] = [adapter_map[m] for m in metrics_order]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "x = np.arange(len(metrics_order))\n",
        "total_variants = len(variant_to_values)\n",
        "width = min(0.8 / total_variants, 0.15)\n",
        "\n",
        "for idx, (label, values) in enumerate(variant_to_values.items()):\n",
        "    offset = (idx - (total_variants - 1) / 2) * width\n",
        "    ax.bar(x + offset, values, width, label=label)\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics_order, rotation=45)\n",
        "ax.set_ylabel(\"Recall (%)\")\n",
        "ax.grid(alpha=0.3, axis=\"y\")\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "# fig.savefig(plot_dir / \"comparison_all.png\")\n",
        "plt.show()\n",
        "\n",
        "summary_path = plot_dir / \"results_summary.csv\"\n",
        "summary_df.to_csv(summary_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
