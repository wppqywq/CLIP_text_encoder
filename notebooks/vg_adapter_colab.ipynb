{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG35vP3oUDaW"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9BrObMUkUDaW"
      },
      "outputs": [],
      "source": [
        "# Parameter block (edit these before running anything else)\n",
        "CONFIG = {\n",
        "    \"repo_url\": \"https://github.com/wppqywq/CLIP_text_encoder.git\",  # GitHub repo to clone\n",
        "    \"repo_branch\": \"main\",\n",
        "    \"repo_dir\": \"/content/comp545_final\",\n",
        "    \"drive_mount\": \"/content/drive\",\n",
        "    \"data_root\": \"/content/drive/MyDrive/comp545_data\", # put ur zip here\n",
        "    \"output_root\": \"/content/drive/MyDrive/comp545_outputs\",\n",
        "    \"packages\": [\n",
        "        \"open-clip-torch\",\n",
        "        \"torch\",\n",
        "        \"torchvision\",\n",
        "        \"pillow\",\n",
        "        \"numpy\",\n",
        "        \"matplotlib\",\n",
        "        \"pandas\",\n",
        "    ],\n",
        "    \"smoke_limit\": 1000,  # number of images for the quick smoke test\n",
        "    \"smoke_distill\": (0.0,),\n",
        "    \"smoke_output\": \"vg_smoke\",\n",
        "    \"full_limit\": None,  # set to None for the full dataset\n",
        "    \"full_distill\": (0.0, 0.1),\n",
        "    \"full_output\": \"vg_full\",\n",
        "    \"adapter_steps\": 300,\n",
        "    \"adapter_batch\": 32,\n",
        "    \"chunk_words\": 8,\n",
        "    \"chunk_stride\": 4,\n",
        "    \"chunk_threshold\": 12,\n",
        "    \"text_pooling\": \"attn\",\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X7Q8LCXyUDaW"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (no-op if already satisfied)\n",
        "if CONFIG[\"packages\"]:\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", *CONFIG[\"packages\"]]\n",
        "    subprocess.run(cmd, check=True)\n",
        "else:\n",
        "    print(\"No extra packages listed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQtlLTuiUDaX",
        "outputId": "0e1766f7-fc2a-48c4-8767-e3cbb0953e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repo already present at /content/comp545_final\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and clone the repository if needed\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "MOUNT_POINT = Path(CONFIG[\"drive_mount\"])\n",
        "if not MOUNT_POINT.is_dir():\n",
        "    drive.mount(str(MOUNT_POINT))\n",
        "\n",
        "repo_dir = Path(CONFIG[\"repo_dir\"]).resolve()\n",
        "if not repo_dir.exists():\n",
        "    repo_dir.parent.mkdir(parents=True, exist_ok=True)\n",
        "    clone_cmd = [\n",
        "        \"git\",\n",
        "        \"clone\",\n",
        "        CONFIG[\"repo_url\"],\n",
        "        str(repo_dir),\n",
        "        \"--branch\",\n",
        "        CONFIG[\"repo_branch\"],\n",
        "        \"--single-branch\",\n",
        "    ]\n",
        "    subprocess.run(clone_cmd, check=True)\n",
        "else:\n",
        "    print(f\"Repo already present at {repo_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNsB-8oaUDaX",
        "outputId": "74679653-188c-4cdd-9718-c0282e1f8a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repo_root: /content/comp545_final\n",
            "data_root: /content/drive/MyDrive/comp545_data\n",
            "output_root: /content/drive/MyDrive/comp545_outputs\n"
          ]
        }
      ],
      "source": [
        "# Configure project paths and ensure modules import correctly\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "repo_root = Path(CONFIG[\"repo_dir\"]).resolve()\n",
        "data_root = Path(CONFIG[\"data_root\"]).resolve()\n",
        "output_root = Path(CONFIG[\"output_root\"]).resolve()\n",
        "\n",
        "os.environ[\"VG_COLAB_REPO_ROOT\"] = str(repo_root)\n",
        "os.environ[\"VG_COLAB_DATA_ROOT\"] = str(data_root)\n",
        "os.environ[\"VG_COLAB_OUTPUT_ROOT\"] = str(output_root)\n",
        "\n",
        "repo_root.mkdir(parents=True, exist_ok=True)\n",
        "data_root.mkdir(parents=True, exist_ok=True)\n",
        "output_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "\n",
        "print(\"repo_root:\", repo_root)\n",
        "print(\"data_root:\", data_root)\n",
        "print(\"output_root:\", output_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "if 0:\n",
        "  # Define paths based on user query\n",
        "  zip_path = Path(\"/content/drive/MyDrive/comp545_data/visual_genome_raw.zip\")\n",
        "  target_dir = Path(\"/content/drive/MyDrive/comp545_data/\")\n",
        "\n",
        "  # Ensure the target directory exists\n",
        "  target_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  if zip_path.is_file():\n",
        "      print(f\"Extracting {zip_path} to {target_dir}\")\n",
        "      with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "          zf.extractall(target_dir)\n",
        "      print(\"Extraction complete.\")\n",
        "  else:\n",
        "      print(f\"Error: Zip file not found at {zip_path}\")"
      ],
      "metadata": {
        "id": "3oFoQnKZYG4n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional helper: unzip image archives from Drive into the expected directory\n",
        "\n",
        "if 0:\n",
        "  zip_map = {\n",
        "      \"images.zip\": data_root / \"visual_genome\" / \"images\",\n",
        "      \"images2.zip\": data_root / \"visual_genome\" / \"images\",\n",
        "  }\n",
        "\n",
        "  for zip_name, target_dir in zip_map.items():\n",
        "      archive_path = data_root / \"visual_genome_raw\" / zip_name\n",
        "      if not archive_path.is_file():\n",
        "          print(f\"Archive not found: {archive_path}\")\n",
        "          continue\n",
        "      target_dir.mkdir(parents=True, exist_ok=True)\n",
        "      with zipfile.ZipFile(archive_path, \"r\") as zf:\n",
        "          print(f\"Extracting {zip_name} -> {target_dir}\")\n",
        "          zf.extractall(target_dir)\n",
        "  print(\"Done extracting image archives.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARNrtZbQYomn",
        "outputId": "15eef59a-47c2-46c7-de26-a29eced6dd73"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting images.zip -> /content/drive/MyDrive/comp545_data/visual_genome/images\n",
            "Extracting images2.zip -> /content/drive/MyDrive/comp545_data/visual_genome/images\n",
            "Done extracting image archives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU4XsiIqUDaX"
      },
      "source": [
        "## Dataset Checklist\n",
        "Ensure Drive contains the Visual Genome archives under `CONFIG['data_root']`:\n",
        "```\n",
        "visual_genome_raw/\n",
        "  region_descriptions.json (or .zip)\n",
        "  image_data.json (or .zip)\n",
        "  VG_100K.zip\n",
        "  VG_100K_2.zip\n",
        "visual_genome/\n",
        "  images/VG_100K/\n",
        "  images/VG_100K_2/\n",
        "```\n",
        "If you already processed splits elsewhere, copy `visual_genome/visual_genome_splits.json` here. Otherwise, run the next cell to prepare everything from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DAHGwIJUDaX",
        "outputId": "27399a05-2c63-44ef-c1fc-8ead3ddc4a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading region_descriptions.json.zip...\n"
          ]
        }
      ],
      "source": [
        "# Optional: download/process/verify Visual Genome (set RUN_PROCESS=True when needed)\n",
        "RUN_PROCESS = 1\n",
        "PROCESS_CONFIG = {\n",
        "    \"max_images\": 5000,\n",
        "    \"max_regions_per_image\": 6,\n",
        "    \"min_region_words\": 3,\n",
        "    \"validation_ratio\": 0.1,\n",
        "    \"test_ratio\": 0.1,\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "if RUN_PROCESS:\n",
        "    from src.config.runtime import resolve_paths\n",
        "    from src.data.visual_genome import (\n",
        "        VisualGenomeProcessConfig,\n",
        "        download_visual_genome,\n",
        "        process_visual_genome,\n",
        "        verify_visual_genome,\n",
        "    )\n",
        "\n",
        "    paths = resolve_paths(\"colab\")\n",
        "    process_cfg = VisualGenomeProcessConfig(**PROCESS_CONFIG)\n",
        "    download_visual_genome(paths, include_images=True, force=False)\n",
        "    processed_path = process_visual_genome(paths, process_cfg)\n",
        "    print(\"Processed splits saved to:\", processed_path)\n",
        "    verify_visual_genome(paths)\n",
        "else:\n",
        "    print(\"Skipping data preparation. Toggle RUN_PROCESS=True if required.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect processed dataset statistics\n",
        "import json\n",
        "\n",
        "splits_path = data_root / \"visual_genome\" / \"visual_genome_splits.json\"\n",
        "if splits_path.is_file():\n",
        "    with splits_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        vg_payload = json.load(f)\n",
        "    counts = vg_payload.get(\"counts\", {})\n",
        "    splits = vg_payload.get(\"splits\", {})\n",
        "    print(\"Processed dataset counts:\", counts)\n",
        "    for name, entries in splits.items():\n",
        "        size = len(entries) if isinstance(entries, list) else 0\n",
        "        print(f\"  {name}: {size} images\")\n",
        "else:\n",
        "    print(f\"Processed splits not found at {splits_path}; run the preparation cell if needed.\")"
      ],
      "metadata": {
        "id": "-VbeBJcEeulp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqNXfYMwUDaX"
      },
      "outputs": [],
      "source": [
        "# Helper utilities for running experiments and plotting metrics\n",
        "from typing import Iterable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "from src.config.runtime import resolve_paths\n",
        "from src.training.vg_adapter import AdapterExperimentConfig, run_visual_genome_adapter\n",
        "\n",
        "\n",
        "def run_experiment(limit, distill_weights: Iterable[float], output_name: str):\n",
        "    config = AdapterExperimentConfig(\n",
        "        output_name=output_name,\n",
        "        limit_images=limit,\n",
        "        distill_weights=tuple(distill_weights),\n",
        "        adapter_steps=CONFIG[\"adapter_steps\"],\n",
        "        adapter_batch=CONFIG[\"adapter_batch\"],\n",
        "        chunk_words=CONFIG[\"chunk_words\"],\n",
        "        chunk_stride=CONFIG[\"chunk_stride\"],\n",
        "        chunk_threshold=CONFIG[\"chunk_threshold\"],\n",
        "        text_pooling=CONFIG[\"text_pooling\"],\n",
        "    )\n",
        "    paths = resolve_paths(\"colab\")\n",
        "    results = run_visual_genome_adapter(paths, config)\n",
        "    metrics_path = results.get(\"metrics_path\")\n",
        "    print(\"Metrics stored at:\", metrics_path)\n",
        "    summary_df = pd.DataFrame(results[\"summary\"])\n",
        "    return results, summary_df\n",
        "\n",
        "\n",
        "def _collect_value_columns(summary_df: pd.DataFrame):\n",
        "    base_cols = [\"baseline\", \"chunk_baseline\"]\n",
        "    adapter_cols = sorted(col for col in summary_df.columns if col.startswith(\"adapter_\"))\n",
        "    return [col for col in base_cols + adapter_cols if col in summary_df]\n",
        "\n",
        "\n",
        "def plot_summary(summary_df: pd.DataFrame, title: str):\n",
        "    value_cols = _collect_value_columns(summary_df)\n",
        "    melted = summary_df.melt(\n",
        "        id_vars=[\"metric\", \"split\"],\n",
        "        value_vars=value_cols,\n",
        "        var_name=\"variant\",\n",
        "        value_name=\"recall\",\n",
        "    )\n",
        "    splits = sorted(melted[\"split\"].unique())\n",
        "    fig, axes = plt.subplots(len(splits), 1, figsize=(8, 4 * len(splits)), sharex=True)\n",
        "    if len(splits) == 1:\n",
        "        axes = [axes]\n",
        "    for ax, split in zip(axes, splits):\n",
        "        sub = melted[melted[\"split\"] == split]\n",
        "        pivot = sub.pivot(index=\"metric\", columns=\"variant\", values=\"recall\")\n",
        "        for variant in pivot.columns:\n",
        "            ax.plot(pivot.index, pivot[variant], marker=\"o\", label=variant)\n",
        "        ax.set_title(f\"{split} split\")\n",
        "        ax.set_ylabel(\"Recall (%)\")\n",
        "        ax.grid(alpha=0.3)\n",
        "    axes[-1].set_xlabel(\"Metric\")\n",
        "    axes[0].legend()\n",
        "    fig.suptitle(title)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_distribution(summary_df: pd.DataFrame, title: str):\n",
        "    value_cols = _collect_value_columns(summary_df)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    for col in value_cols:\n",
        "        plt.hist(summary_df[col], bins=10, alpha=0.4, label=col)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Recall (%)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_loss_logs(results: dict, title: str):\n",
        "    loss_logs = results.get(\"loss_logs\")\n",
        "    if not loss_logs:\n",
        "        print(\"No loss logs recorded.\")\n",
        "        return\n",
        "    fig, (ax_loss, ax_scale) = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
        "    for weight_key, entries in loss_logs.items():\n",
        "        if not entries:\n",
        "            continue\n",
        "        df = pd.DataFrame.from_records(entries, columns=[\"step\", \"loss\", \"scale\"])\n",
        "        if df.empty:\n",
        "            continue\n",
        "        label = f\"distill={weight_key}\"\n",
        "        ax_loss.plot(df[\"step\"], df[\"loss\"], marker=\"o\", label=label)\n",
        "        ax_scale.plot(df[\"step\"], df[\"scale\"], marker=\"o\", label=label)\n",
        "    ax_loss.set_title(\"Adapter loss\")\n",
        "    ax_loss.set_xlabel(\"Step\")\n",
        "    ax_loss.set_ylabel(\"Loss\")\n",
        "    ax_loss.grid(alpha=0.3)\n",
        "    ax_scale.set_title(\"Logit scale\")\n",
        "    ax_scale.set_xlabel(\"Step\")\n",
        "    ax_scale.set_ylabel(\"Scale\")\n",
        "    ax_scale.grid(alpha=0.3)\n",
        "    ax_loss.legend()\n",
        "    fig.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def summarize_best_metrics(summary_df: pd.DataFrame):\n",
        "    if summary_df.empty:\n",
        "        print(\"Summary is empty.\")\n",
        "        return\n",
        "    best_rows = summary_df.loc[summary_df.groupby(\"metric\")[\"adapter\"].idxmax()].sort_values(\"metric\")\n",
        "    display(best_rows[[\"metric\", \"distill\", \"adapter\", \"baseline_original\", \"baseline_chunk\"]])\n",
        "\n",
        "\n",
        "def show_test_metrics(results: dict, weight: float):\n",
        "    adapter_metrics = results.get(\"adapter_metrics\", {})\n",
        "    block = adapter_metrics.get(weight)\n",
        "    if not block:\n",
        "        print(f\"No metrics found for distill={weight}.\")\n",
        "        return\n",
        "    print(f\"Test metrics for distill={weight}:\")\n",
        "    test_df = pd.DataFrame.from_dict(block.get(\"test\", {}), orient=\"index\")\n",
        "    display(test_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "1HQPc8Wje05M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvfl7WuQUDaX"
      },
      "source": [
        "## 3. Smoke Test\n",
        "Run a smaller experiment to confirm everything is wired correctly. This keeps runtime manageable before committing to the full dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JHoqkn4H5bjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLZpqcTbUDaX"
      },
      "outputs": [],
      "source": [
        "smoke_results, smoke_summary = run_experiment(\n",
        "    limit=CONFIG[\"smoke_limit\"],\n",
        "    distill_weights=CONFIG[\"smoke_distill\"],\n",
        "    output_name=CONFIG[\"smoke_output\"],\n",
        ")\n",
        "print(\"Smoke test metrics head:\")\n",
        "print(smoke_summary.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_best_metrics(smoke_summary.loc[smoke_summary[\"split\"] == \"test\"])\n",
        "plot_loss_logs(smoke_results, title=\"Smoke Test Adapter Dynamics\")\n",
        "if CONFIG[\"smoke_distill\"]:\n",
        "    show_test_metrics(smoke_results, CONFIG[\"smoke_distill\"][-1])"
      ],
      "metadata": {
        "id": "PNHQfFTve3kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSYTzfXtUDaX"
      },
      "outputs": [],
      "source": [
        "plot_summary(smoke_summary, title=\"Smoke Test Recall Curves\")\n",
        "plot_distribution(smoke_summary, title=\"Smoke Test Recall Distribution\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwUcCDFPUDaX"
      },
      "source": [
        "## 4. Full Experiment\n",
        "Once the smoke test looks good, launch the full dataset run below. This may take significantly longer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izwm2fCJUDaX"
      },
      "outputs": [],
      "source": [
        "full_results, full_summary = run_experiment(\n",
        "    limit=CONFIG[\"full_limit\"],\n",
        "    distill_weights=CONFIG[\"full_distill\"],\n",
        "    output_name=CONFIG[\"full_output\"],\n",
        ")\n",
        "print(\"Full run metrics head:\")\n",
        "print(full_summary.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_best_metrics(full_summary.loc[full_summary[\"split\"] == \"test\"])\n",
        "plot_loss_logs(full_results, title=\"Full Experiment Adapter Dynamics\")\n",
        "if CONFIG[\"full_distill\"]:\n",
        "    show_test_metrics(full_results, CONFIG[\"full_distill\"][-1])"
      ],
      "metadata": {
        "id": "iG6d7s2Ke7Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MytlHlCcUDaX"
      },
      "outputs": [],
      "source": [
        "plot_summary(full_summary, title=\"Full Experiment Recall Curves\")\n",
        "plot_distribution(full_summary, title=\"Full Experiment Recall Distribution\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaVy5IXGUDaX"
      },
      "source": [
        "## 5. Compare Smoke vs Full\n",
        "The cell below merges both runs (if available) to compare adapter performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiAJMrh6UDaX"
      },
      "outputs": [],
      "source": [
        "def merge_runs(smoke_df: pd.DataFrame, full_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    smoke_df = smoke_df.copy()\n",
        "    smoke_df[\"run\"] = \"smoke\"\n",
        "    full_df = full_df.copy()\n",
        "    full_df[\"run\"] = \"full\"\n",
        "    return pd.concat([smoke_df, full_df], ignore_index=True)\n",
        "\n",
        "if \"smoke_summary\" in globals() and \"full_summary\" in globals():\n",
        "    combined = merge_runs(smoke_summary, full_summary)\n",
        "    display(combined.head())\n",
        "    value_cols = _collect_value_columns(smoke_summary)\n",
        "    for split in sorted(combined[\"split\"].unique()):\n",
        "        subset = combined[combined[\"split\"] == split]\n",
        "        fig, ax = plt.subplots(figsize=(10, 4))\n",
        "        positions = list(range(len(value_cols)))\n",
        "        width = 0.35\n",
        "        smoke_vals = [float(subset[subset[\"run\"] == \"smoke\"][col].mean()) for col in value_cols]\n",
        "        full_vals = [float(subset[subset[\"run\"] == \"full\"][col].mean()) for col in value_cols]\n",
        "        ax.bar([p - width / 2 for p in positions], smoke_vals, width=width, label=\"smoke\")\n",
        "        ax.bar([p + width / 2 for p in positions], full_vals, width=width, label=\"full\")\n",
        "        ax.set_xticks(positions)\n",
        "        ax.set_xticklabels(value_cols, rotation=45)\n",
        "        ax.set_ylabel(\"Mean Recall (%)\")\n",
        "        ax.set_title(f\"Smoke vs Full comparison ({split} split)\")\n",
        "        ax.grid(alpha=0.3, axis=\"y\")\n",
        "        ax.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Both smoke_summary and full_summary must be available to compare.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdate3qGUDaX"
      },
      "source": [
        "## 6. Wrap Up\n",
        "- Push code changes back to GitHub once satisfied.\n",
        "- Large data stays on Drive; processed results land under `CONFIG['output_root']`.\n",
        "- Adjust the configuration cell at the top to try different adapters, chunking strategies, or distillation weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kHbTvbgqZvBi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}